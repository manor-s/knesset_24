{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manor-s/knesset_24/blob/main/Data_Science_Course_Project_Regression_Manor_Shpriz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye_cPymylkb4"
      },
      "source": [
        "<font size=\"6\">Instructions of Use.</font>\n",
        "\n",
        "* Upload Knesset Protocol .csv file to Files section.\n",
        "* Run the cell below.\n",
        "* Create a new cell, from there you can run the code:\n",
        "```\n",
        "run_model(\"your_file_name.csv\")\n",
        "```\n",
        "* The prediction of likud lines is given as a printed output, and also as a returned value, for your convenience.\n",
        "* Thank You.\n",
        "\n",
        "<br>\n",
        "\n",
        "<font size=\"6\">Model Building Methodology.</font>\n",
        "\n",
        "* Grouping by subject.\n",
        "* Exctracting total lines for each subject.\n",
        "* Calculating sum of words for subject.\n",
        "* Calculating number of lines by selected parties for each subject.\n",
        "* Calculating number of references from chair of likud members (most probably invites).\n",
        "* Calculating number of refernces from formal speaker (most probably dialogue) of likud members.\n",
        "* Categorizing the subject (only one category was shown to be siginficant) and creating an index of total lines per subject.\n",
        "* Calaculating appearance of important words for subject (seperately), word that were gathered by simple nlp processing, and found to be with high corr to likud lines.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.linear_model import ElasticNet\n",
        "import pickle\n",
        "import requests\n",
        "\n",
        "\n",
        "#download model\n",
        "url = \"https://drive.google.com/uc?id=1QxEkkaGQNVERLeB6O3aj6ES32EgiX6jJ\"\n",
        "\n",
        "response = requests.get(url)\n",
        "metadata = pickle.loads(response.content)  # Load directly from response\n",
        "\n",
        "reg_model = metadata[\"model\"]\n",
        "\n",
        "\n",
        "#target function\n",
        "def run_model(filename):\n",
        "    test_df = pd.read_csv(filename)\n",
        "    research_file = make_research_file(test_df)\n",
        "    likud_lines = calculate_likud_lines(research_file, reg_model)\n",
        "    print(f\"Likud lines: {likud_lines}\")\n",
        "    return likud_lines\n",
        "\n",
        "\n",
        "#invites of likud members from text column\n",
        "def count_likud_invites(text, mk_data):\n",
        "    count = 0\n",
        "    seen_mentions = set()  # Track processed MKs to avoid duplicates\n",
        "\n",
        "    # Find all last names in the dataset to check for ambiguity\n",
        "    all_last_names = mk_data[\"last_name\"].value_counts()\n",
        "\n",
        "    for _, mk in mk_data.iterrows():\n",
        "        if mk[\"party\"] == \"הליכוד\":\n",
        "            # Adjust titles based on gender, including \"ה\" prefix\n",
        "            titles = [\"חבר כנסת\", \"חבר הכנסת\"] if mk[\"gender\"] == \"male\" else [\"חברת כנסת\", \"חברת הכנסת\"]\n",
        "\n",
        "            # Generate patterns for full names\n",
        "            full_name_patterns = [\n",
        "                f\"{title} {mk['first_name']} {mk['last_name']}\" for title in titles\n",
        "            ] + [\n",
        "                f\"{title} {mk['middle_name']} {mk['last_name']}\" for title in titles if mk['middle_name']\n",
        "            ]\n",
        "\n",
        "            # Generate patterns for last names\n",
        "            last_name_patterns = [f\"{title} {mk['last_name']}\" for title in titles] + [f\"{mk['last_name']}\"]\n",
        "\n",
        "            # Check full name patterns first\n",
        "            for pattern in full_name_patterns:\n",
        "                if mk['last_name'] in seen_mentions:  # Skip if already matched\n",
        "                    break\n",
        "                if re.search(pattern, text):\n",
        "                    count += 1\n",
        "                    seen_mentions.add(mk['last_name'])\n",
        "                    break  # Prioritize full name match\n",
        "\n",
        "            # Check last name patterns as a fallback\n",
        "            if mk['last_name'] not in seen_mentions:\n",
        "                # Validate last name is not ambiguous\n",
        "                if all_last_names[mk['last_name']] == 1:  # Ensure it's unique to Likud\n",
        "                    for pattern in last_name_patterns:\n",
        "                        if re.search(pattern, text):\n",
        "                            count += 1\n",
        "                            seen_mentions.add(mk['last_name'])\n",
        "                            break  # Stop after matching last name\n",
        "\n",
        "    return count\n",
        "\n",
        "\n",
        "#looser refferals of likud members from text column\n",
        "def count_likud_refs(text, mk_data):\n",
        "    count = 0\n",
        "    seen_mentions = set()  # Track processed MKs to avoid duplicates\n",
        "\n",
        "    # Handle cases where text is null or NaN\n",
        "    if not isinstance(text, str):\n",
        "        return 0\n",
        "\n",
        "    # Find all last names and first names in the dataset to check for ambiguity\n",
        "    all_last_names = mk_data[\"last_name\"].value_counts()\n",
        "    all_first_names = mk_data[\"first_name\"].value_counts()\n",
        "\n",
        "    for _, mk in mk_data.iterrows():\n",
        "        if mk[\"party\"] == \"הליכוד\":\n",
        "            # Adjust titles based on gender, including \"ה\" prefix\n",
        "            titles = [\"חבר כנסת\", \"חבר הכנסת\"] if mk[\"gender\"] == \"male\" else [\"חברת כנסת\", \"חברת הכנסת\"]\n",
        "\n",
        "            # Generate patterns for full names\n",
        "            full_name_patterns = [\n",
        "                f\"{title} {mk['first_name']} {mk['last_name']}\" for title in titles\n",
        "            ] + [\n",
        "                f\"{title} {mk['middle_name']} {mk['last_name']}\" for title in titles if mk['middle_name']\n",
        "            ]\n",
        "\n",
        "            # Generate patterns for last names\n",
        "            last_name_patterns = [f\"{title} {mk['last_name']}\" for title in titles] + [f\"{mk['last_name']}\"]\n",
        "\n",
        "            # Generate patterns for first names as a fallback\n",
        "            first_name_patterns = [f\"{title} {mk['first_name']}\" for title in titles] + [f\"{mk['first_name']}\"]\n",
        "\n",
        "            # Check full name patterns first\n",
        "            for pattern in full_name_patterns:\n",
        "                if mk['last_name'] in seen_mentions:  # Skip if already matched\n",
        "                    break\n",
        "                if re.search(pattern, text):\n",
        "                    count += 1\n",
        "                    seen_mentions.add(mk['last_name'])\n",
        "                    break  # Prioritize full name match\n",
        "\n",
        "            # Check last name patterns as a fallback\n",
        "            if mk['last_name'] not in seen_mentions:\n",
        "                # Validate last name is not ambiguous\n",
        "                if all_last_names[mk['last_name']] == 1:  # Ensure it's unique to Likud\n",
        "                    for pattern in last_name_patterns:\n",
        "                        if re.search(pattern, text):\n",
        "                            count += 1\n",
        "                            seen_mentions.add(mk['last_name'])\n",
        "                            break  # Stop after matching last name\n",
        "\n",
        "            # Check first name patterns as the final fallback\n",
        "            if mk['last_name'] not in seen_mentions and mk['first_name'] not in seen_mentions:\n",
        "                # Validate first name is not ambiguous\n",
        "                if all_first_names[mk['first_name']] == 1:  # Ensure it's unique to Likud\n",
        "                    for pattern in first_name_patterns:\n",
        "                        if re.search(pattern, text):\n",
        "                            count += 1\n",
        "                            seen_mentions.add(mk['first_name'])  # Mark first name as matched\n",
        "                            break  # Stop after matching first name\n",
        "\n",
        "    return count\n",
        "\n",
        "\n",
        "#classification of subject column (adjusted to the only ane with corr > 0.5 with likud lines in train set)\n",
        "def classify_subject(subject):\n",
        "    if re.search(r\"חייל|צבא|בטחון|טרור|שב\\\"כ|עימותים|הטרור|המינהל האזרחי|אלימות\", subject):\n",
        "        return \"Defense_Security\"\n",
        "\n",
        "\n",
        "#count word occurences in text\n",
        "def count_word_occurrences(text, word):\n",
        "    pattern = rf'{re.escape(word)}'\n",
        "    return len(re.findall(pattern, text))\n",
        "\n",
        "\n",
        "#research set maker from basic protocol file\n",
        "def make_research_file(protocols):\n",
        "\n",
        "    mk_data = pd.read_csv(\"https://drive.google.com/uc?id=1cfjQNsuYScMnyqyeJ6y1MtJ7rE87wm-2\")\n",
        "\n",
        "    if protocols[\"subject\"].notna().any():\n",
        "        most_common_subject = protocols[\"subject\"].mode().iloc[0]  # Find the most common subject\n",
        "        protocols[\"subject\"] = protocols[\"subject\"].fillna(most_common_subject)\n",
        "    else:\n",
        "        # If all values are missing, fill with \"null\"\n",
        "        protocols[\"subject\"] = \"null\"\n",
        "\n",
        "    research_data = pd.DataFrame(protocols[\"subject\"].unique(), columns=[\"subject\"])\n",
        "    filtered_protocols = protocols.copy()\n",
        "    filtered_protocols[\"word_count\"] = filtered_protocols[\"text\"].apply(lambda x: len(str(x).split()))\n",
        "    filtered_lines = filtered_protocols.groupby(\"subject\").size().reset_index(name=\"filtered_lines\")\n",
        "    research_data = research_data.merge(filtered_lines, on=\"subject\", how=\"left\")\n",
        "    filtered_lines = filtered_protocols.groupby(\"subject\")[\"word_count\"].sum().reset_index(name=\"word_count\")\n",
        "    research_data = research_data.merge(filtered_lines, on=\"subject\", how=\"left\")\n",
        "    research_data[\"filtered_lines\"] = research_data[\"filtered_lines\"].fillna(0).astype(int)\n",
        "\n",
        "\n",
        "    # lines by party\n",
        "\n",
        "    data = filtered_protocols.copy()\n",
        "\n",
        "    # List of parties with >0.5 corr with likud lines\n",
        "    parties = ['הציונות הדתית', 'הרשימה המשותפת', 'יש עתיד', 'ישראל ביתנו', 'כחול לבן', 'ש\"ס']\n",
        "\n",
        "    # Group data by subject and count occurrences of each party\n",
        "    grouped_data = data.groupby(\"subject\")[\"party\"].value_counts().unstack(fill_value=0).reset_index()\n",
        "\n",
        "    # Ensure all parties are in the columns, even if they don't appear in the data\n",
        "    for party in parties:\n",
        "        if party not in grouped_data.columns:\n",
        "            grouped_data[party] = 0\n",
        "\n",
        "    # Reorder columns (optional) to ensure consistency with the party list\n",
        "    grouped_data = grouped_data[[\"subject\"] + parties]\n",
        "\n",
        "    research_data = research_data.merge(grouped_data, on=\"subject\", how=\"left\")\n",
        "\n",
        "\n",
        "    # Likud members invites by chairperson\n",
        "\n",
        "    data = filtered_protocols[filtered_protocols[\"role\"].isin(['יו\"ר', 'היו\"ר', 'יו\"ר הכנסת'])].copy()\n",
        "\n",
        "    if data.empty:\n",
        "        # Create an empty grouped_data DataFrame with the same structure\n",
        "        grouped_data = pd.DataFrame(columns=[\"subject\", \"likud_invites\"])\n",
        "    else:\n",
        "        # Apply function to count invites\n",
        "        data[\"likud_invites\"] = data[\"text\"].apply(lambda x: count_likud_invites(x, mk_data))\n",
        "\n",
        "    # Group by subject and calculate likud_invites\n",
        "    grouped_data = data.groupby(\"subject\", as_index=False).agg({\"likud_invites\": \"sum\"})\n",
        "\n",
        "    research_data = research_data.merge(grouped_data, on=\"subject\", how=\"left\")\n",
        "\n",
        "\n",
        "    # mentions of Likud MKs by formal speaker\n",
        "\n",
        "    data = filtered_protocols[\n",
        "    (filtered_protocols[\"role\"].notnull()) &\n",
        "    (~filtered_protocols[\"role\"].isin(['יו\"ר', 'היו\"ר', 'יו\"ר הכנסת']))\n",
        "    ].copy()\n",
        "\n",
        "    if data.empty:\n",
        "        # Create an empty grouped_data DataFrame with the required structure\n",
        "        grouped_data = pd.DataFrame(columns=[\"subject\", \"likud_refs\"])\n",
        "    else:\n",
        "        # Apply the function to count Likud references\n",
        "        data[\"likud_refs\"] = data[\"text\"].apply(lambda x: count_likud_refs(x, mk_data))\n",
        "\n",
        "        # Group by subject and calculate Likud references\n",
        "        grouped_data = (\n",
        "            data.groupby(\"subject\", as_index=False)\n",
        "            .agg(likud_refs=(\"likud_refs\", \"sum\"))\n",
        "        )\n",
        "\n",
        "    research_data = research_data.merge(grouped_data, on=\"subject\", how=\"left\")\n",
        "\n",
        "    research_data[\"likud_refs\"] = research_data[\"likud_refs\"].fillna(0).astype(int)\n",
        "\n",
        "\n",
        "    # Categorize subjects and compute category-specific scores based on the number of lines\n",
        "\n",
        "    data = filtered_protocols.copy()\n",
        "    data[\"category\"] = data[\"subject\"].apply(classify_subject)\n",
        "    data[\"lines\"] = 1\n",
        "\n",
        "    pivot_table = data.pivot_table(\n",
        "        index=\"subject\",\n",
        "        columns=\"category\",\n",
        "        values=\"lines\",\n",
        "        aggfunc=\"sum\",\n",
        "        fill_value=0\n",
        "    )\n",
        "\n",
        "    expected_categories = [\"Defense_Security\"]  # handle missing categories\n",
        "    for category in expected_categories:\n",
        "        if category not in pivot_table.columns:\n",
        "            pivot_table[category] = 0\n",
        "\n",
        "\n",
        "    pivot_table.columns = [f\"subject_category_{col}\" for col in pivot_table.columns]\n",
        "    ft = pivot_table.reset_index()\n",
        "\n",
        "    research_data = research_data.merge(ft,  on=\"subject\", how=\"left\")\n",
        "\n",
        "\n",
        "    # Count base words occurances\n",
        "\n",
        "    word_list = [\n",
        "        \"חבר\", \"ישראל\", \"מדינה\", \"ממשלה\", \"שר\",\n",
        "        \"חשוב\", \"אזרח\", \"ערבי\", \"כבוד\", \"חייל\", \"משפט\", \"ביטחון\",\n",
        "        \"קואליציה\"\n",
        "        ]\n",
        "\n",
        "    data = filtered_protocols[~filtered_protocols[\"role\"].isin(['יו\"ר', 'היו\"ר', 'יו\"ר הכנסת'])].copy()\n",
        "\n",
        "    if data.empty:\n",
        "        # Create an empty grouped_data DataFrame with the required structure\n",
        "        grouped_data = pd.DataFrame(columns=[\"subject\"] + [f\"count_{word}\" for word in word_list])\n",
        "    else:\n",
        "        # Ensure text column is non-null and of string type\n",
        "        data[\"text\"] = data[\"text\"].fillna(\"\").astype(str)\n",
        "\n",
        "        # Group by subject and aggregate text\n",
        "        grouped_data = data.groupby(\"subject\")[\"text\"].apply(\" \".join).reset_index()\n",
        "\n",
        "        # Create a column for each word and count its occurrences in the aggregated \"text\" column\n",
        "        for word in word_list:\n",
        "            grouped_data[f\"count_{word}\"] = grouped_data[\"text\"].apply(lambda x: count_word_occurrences(x, word))\n",
        "\n",
        "        # Drop the aggregated text column\n",
        "        grouped_data = grouped_data.drop(columns=[\"text\"])\n",
        "\n",
        "    research_data = research_data.merge(grouped_data, on=\"subject\", how=\"left\")\n",
        "\n",
        "    research_data = research_data.fillna(0)\n",
        "\n",
        "\n",
        "    corr_matrix = research_data.drop([\"subject\"], axis=1).corr()\n",
        "\n",
        "    #output of pd_to_investigate\n",
        "    return research_data\n",
        "\n",
        "\n",
        "\n",
        "def calculate_likud_lines(research_data, model):\n",
        "\n",
        "    y_pred = model.predict(research_data.drop(columns=[\"subject\"], axis=1))\n",
        "\n",
        "    y_pred = np.where(y_pred < 0, 0, y_pred)\n",
        "\n",
        "    return y_pred.sum()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V5TBZZjF9W0N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYrWcLP2A1H9MxP+0Zl61X",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}